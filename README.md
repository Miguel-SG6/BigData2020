# BigData2020UTM

## Big data class

### syllabus contents
* The importance of data
Your data without important because if someone gets your data. You can impersonate your identity or even get your bank account and grab the money. Now that your data is already becoming digital, they have become much more important.

* Big Data
It is the storage or data set whose volume, speed, processing are much larger than a database

* Difference between private and open data
Virtually open data is easily found while private data is not.

* Difference between structured and unstructured data
Structured data is data that is easy to search while unstructured data is difficult to search as well as multimedia files.

* Difference between stored and moving data
The stored data are structured and generate information and the moving data are the ones that are circulating in the network

* Data analysis
It is responsible for examining the data in order to draw a conclusion about the information

* Impact of data analytics
The impact it has is that companies can now earn income through data analysis

* Different types of data analytics
-Descriptive: In a business it allows you to see the main metrics within the business. For example, gains and losses in the month, sales made, etc.
-Diagnostic: You must have the necessary tools so that the analyst can deepen the data and isolate the root cause of a problem.
-Predictive: Predictive analysis has to do with prediction. Either the probability of an event occurring in the future, the forecast of a quantifiable amount or the estimation of a point in time at which something could happen - all of them are done through predictive models.
-Prescriptive: The prescriptive model uses an understanding of what has happened, why it has happened and a variety of "what could happen" analysis to help the user determine the best course of action to take. The prescriptive analysis is usually not only with an individual action, but in fact it is a series of other actions.

### other topics

* MapReduce is a framework that provides a parallel and distributed data processing system and is aimed at solving problems with large datasets, so it uses the HDFS distributed file system.

* Hoodoop: Simply put, Hadoop is a set of open source programs and procedures that anyone can use as the “backbone” of their Big Data operations
-Ability to store and process large quantities
-Fault tolerant
-It is very flexible
-Low cost
-Scalable

* Apache Spark: The idea is that we have n machines, for example ten machines, and each of those instances will have a version of Apache Spark installed. In this way, when we have to process a large amount of data, for example a very large file, we can divide it into ten parts, and each machine will handle a tenth of the file, and in the end we will join it.

* Lamba and Kappa
Its objective was to have a robust fault-tolerant system, both human and hardware, that was linearly scalable and that allowed writing and reading with low latency, while kappa points to "weak" Lambda Architecture and how to solve them through an evolution . Your proposal is to eliminate the batch layer leaving only the streaming layer.

* Docker: The idea behind Docker is to create lightweight and portable containers for software applications that can run on any machine with Docker installed, thus also facilitating the deployment or execution of the software.

### Github commands

_this is cursive_


```
git config --global user.email miguelsalvgomez@gmail.com

git init index.html

git add temp.txt

git clone alex@93.188.160.58:/path/to/repository

git commit –m “Cambie un boton”

git log

```



